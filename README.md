# session

# Разработка приложений с системами искусственного интеллекта - Сессия дайындығы

## 1-блок сұрақтары

### 1. Жасанды нейрондық желілердің негізгі қағидаларын сипаттаңыз
**Жауап:** Нейрондық желілер - бұл адам миының жұмысын еліктейтін жүйе. Негізгі қағидалары:
- **Нейрондар** - ақпаратты өңдейтін бірліктер
- **Салмақтар** - байланыстардың күші
- **Белсендіру функциясы** - шығысты есептейді
- **Оқыту** - қателіктерді азайту

**Мысал:** Бір нейрон: Кіріс (x1=2, x2=3) → Салмақтар (w1=0.5, w2=0.3) → Шығыс = 2*0.5 + 3*0.3 = 1.9

### 2. Персептрон моделінің құрылымын түсіндіріңіз
**Жауап:** Персептрон - ең қарапайым нейрондық желі. Құрылымы:
- **Кіріс қабаты** - деректерді қабылдайды
- **Салмақтар** - әрбір кіріске көбейтіледі
- **Қосындылау** - барлық көбейтінділерді қосады
- **Белсендіру** - шығысты шешімге айналдырады

**Мысал:** Кіріс: [1, 0], Салмақ: [0.6, 0.4], Bias: 0.1 → Шығыс = 1*0.6 + 0*0.4 + 0.1 = 0.7

### 3. Көпқабатты персептронның жұмыс принципін баяндаңыз
**Жауап:** MLP - бірнеше қабаттан тұрады:
- **Кіріс қабаты** - деректер
- **Жасырын қабаттар** - ерекшеліктерді үйренеді
- **Шығыс қабаты** - нәтиже

Ақпарат алдыға қарай өтіп, қатені кері таратады.

**Мысал:** Кіріс [2,3] → Жасырын [h1, h2] → Шығыс [0.8] (иттің суреті)

### 4. Белсендіру функцияларының мәнін ашыңыз
**Жауап:** Белсендіру функциясы - сызықтық емес шешім қабылдауға көмектеседі. Ол нейронның қандай сигнал беретінін анықтайды.

**Мысал:** 
- Кіріс = 0.5 → ReLU(0.5) = 0.5
- Кіріс = -0.3 → ReLU(-0.3) = 0

### 5. Sigmoid және ReLU функцияларының айырмашылығын көрсетіңіз
**Жауап:** 
- **Sigmoid:** σ(x) = 1/(1+e^(-x)), шығыс [0,1] аралығында
  - Мысал: σ(0) = 0.5, σ(2) = 0.88
- **ReLU:** f(x) = max(0, x), теріс мәндерді 0-ге айналдырады
  - Мысал: ReLU(3) = 3, ReLU(-2) = 0

**Артықшылық:** ReLU жылдам оқытады, Sigmoid бинарлы классификацияда жақсы.

### 6. Нейрондық желілердегі қателік есептеу тәсілін сипаттаңыз
**Жауап:** Қателік = нақты мән - болжам. Негізгі функциялар:
- **MSE (Mean Squared Error):** (1/n)Σ(y_нақты - y_болжам)²
- **Cross-Entropy:** -Σ y*log(ŷ)

**Мысал:** Нақты=[1,0], Болжам=[0.9,0.1] → Қателік=(1-0.9)²+(0-0.1)²=0.02

### 7. Кері тарату (backpropagation) алгоритмінің кезеңдерін түсіндіріңіз
**Жауап:** Кезеңдері:
1. **Алға тарату** - болжам жасау
2. **Қателікті есептеу** - нақты мәнмен салыстыру
3. **Градиент есептеу** - әрбір салмақтың қателікке әсері
4. **Салмақтарды жаңарту** - w = w - α*градиент

**Мысал:** Қателік=0.5 → Градиент=0.2 → Жаңа салмақ = 0.6 - 0.01*0.2 = 0.598

### 8. Градиентті төмендету әдісінің логикасын сипаттаңыз
**Жауап:** Қателікті азайту үшін тау етегіне түсу сияқты. Формула: w_жаңа = w_ескі - α*∂Loss/∂w

**Мысал:** Салмақ=1.0, Оқыту жылдамдығы=0.1, Градиент=0.5 → Жаңа салмақ=1.0-0.1*0.5=0.95

### 9. TensorFlow кітапханасының архитектурасын баяндаңыз
**Жауап:** TensorFlow - Google-дың ML фреймворкі:
- **Keras API** - жоғары деңгейлі, оңай
- **Tensors** - көп өлшемді массивтер
- **Граф** - есептеу операциялары
- **Сессия** - модельді орындау

**Мысал:** `model = tf.keras.Sequential([Dense(10)])`

### 10. Модельді компиляциялау мен оқыту процестерін баяндаңыз
**Жауап:** 
**Компиляция:** `model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])`
- Optimizer - салмақтарды жаңарту
- Loss - қателік функциясы
- Metrics - бағалау

**Оқыту:** `model.fit(X_train, y_train, epochs=10, batch_size=32)`

**Мысал:** 100 деректі 10 рет оқытамыз, 32-ден топтап.

### 11. Нейрондық желілердегі overfitting және underfitting құбылыстарын түсіндіріңіз
**Жауап:**
- **Overfitting** - модель тек оқыту деректерін жаттап алады, жаңа деректерде нашар
- **Underfitting** - модель өте қарапайым, ештеңе үйренбейді

**Мысал:** Overfitting: оқытуда 99%, тестте 60%. Underfitting: оқытуда 50%, тестте 48%.

**Шешім:** Dropout, регуляризация, көбірек деректер.

### 12. Регрессия әдістерінің мәнін түсіндіріңіз
**Жауап:** Регрессия - сандық мәнді болжау.
- **Сызықтық:** y = mx + b (түзу сызық)
- **Сызықтық емес:** қисық, полином

**Мысал:** Үйдің бағасын болжау: бөлме саны (x) → баға (y). x=3 → y=150000$

### 13. Логистикалық регрессияның негізгі идеясы мен қолдану салаларын баяндаңыз
**Жауап:** Бинарлы классификация (Иә/Жоқ). Sigmoid функциясын қолданады: P = 1/(1+e^(-z))

**Қолдану:** 
- Email спам ба?
- Аурудың бар ма?

**Мысал:** z=2 → P=1/(1+e^(-2))=0.88 → Спам (>0.5)

### 14. Кластерлеу әдістерінің мәнін түсіндіріңіз
**Жауап:** Ұқсас деректерді топқа бөлу (unsupervised learning). Белгісіз топтарды табады.

**Мысал:** Тұтынушыларды 3 топқа бөлу: белсенді, орташа, пассивті.

### 15. K-Means артықшылықтары мен кемшіліктерін баяндаңыз
**Жауап:**
**Артықшылықтары:**
- Жылдам әрі қарапайым
- Үлкен деректермен жұмыс істейді

**Кемшіліктері:**
- K параметрін алдын ала білу керек
- Outlier-ларға сезімтал

**Мысал:** 1000 клиент → 3 минутта 5 кластерге бөледі.

### 16. K-Means алгоритмнің негізгі қадамдарын сипаттаңыз
**Жауап:**
1. **Бастапқы центрлер** - кездейсоқ K нүкте таңдау
2. **Тағайындау** - әрбір нүктені ең жақын центрге бөлу
3. **Жаңарту** - әрбір кластердің ортасын есептеу
4. **Қайталау** - центрлер өзгермегенше

**Мысал:** K=2, нүктелер (1,2), (2,3), (8,9) → Центрлер: (1.5, 2.5) және (8, 9)

### 17. k-NN (k-Nearest Neighbors) әдісінің принципін түсіндіріңіз
**Жауап:** Ең жақын K көршіге қарап класс анықтайды.

**Қадамдар:**
1. Қашықтықты есептеу
2. K ең жақынын табу
3. Көпшілік дауыс беруі

**Мысал:** K=3, жаңа нүкте (5,5). Көршілер: 2 қызыл, 1 көк → Нәтиже: қызыл

### 18. Decision Tree алгоритмінің принципін түсіндіріңіз
**Жауап:** Шешім ағашы - сұрақтар арқылы шешім қабылдайды.

**Құрылым:**
- **Түйін** - сұрақ (мысал: Жас > 30?)
- **Бұтақ** - жауап (Иә/Жоқ)
- **Жапырақ** - соңғы шешім

**Мысал:** Несие беру ма? → Табыс>5000? Иә → ЖА, Жоқ → Жұмыс>2 жыл? ...

### 19. Деректерді алдын ала өңдеудің (Preprocessing) мәні мен мақсатын түсіндіріңіз
**Жауап:** Деректерді модельге жарамды түрге келтіру:
- **Тазалау** - жоғалған мәндер, қателер
- **Нормализация** - бірдей масштаб
- **Кодтау** - мәтінді санға

**Мысал:** Жас [20, 60], Табыс [1000, 100000] → Normalization → [0-1] аралығы

### 20. Терең оқыту (Deep Learning) әдістерінің негізгі ерекшеліктерін сипаттаңыз
**Жауап:** Көп қабатты нейрондық желілер:
- 10+ жасырын қабат
- Күрделі патернді үйренеді
- Көп деректер қажет

**Мысал:** Суретті тану: 50 қабат → мысық деп анықтайды, 95% дәлдік.

### 21. CNN (Convolutional Neural Network) құрылымы мен жұмыс принципін түсіндіріңіз
**Жауап:** Сурет үшін арнайы желі:
- **Convolution қабаты** - ерекшеліктерді табады (шеттер, бұрыштар)
- **Pooling** - өлшемді кішірейтеді
- **Dense** - классификация

**Мысал:** 28x28 сурет → Conv → 14x14 → Pooling → 7x7 → Dense → "8" цифры

### 22. RNN (Recurrent Neural Network) құрылымы мен жұмыс принципін түсіндіріңіз
**Жауап:** Тізбекті деректерге арналған, жадыны пайдаланады.

**Құрылым:** Бір қабат өзіне қайтып қосылады, алдыңғы ақпаратты есте сақтайды.

**Мысал:** "Мен сөйлеймін ___" → RNN алдыңғы сөздерді есте → "қазақша" деп болжайды.

### 23. Жасанды интеллект (ЖИ) этикасының негізгі принциптерін сипаттаңыз
**Жауап:**
- **Әділдік** - барлығына бірдей қатынас
- **Қауіпсіздік** - зиян келтірмеу
- **Ашықтық** - түсінікті шешімдер
- **Жауапкершілік** - қателіктер үшін

**Мысал:** Жұмысқа қабылдау AI - барлық нәсілге әділ болуы керек.

### 24. Bias (біржақтылық) ұғымын түсіндіріңіз. Мысалдар келтіріңіз
**Жауап:** Модель бір топқа артықшылық береді.

**Мысалдар:**
- Жұмысқа AI - әйелдерді кем бағалайды (деректерде ер адамдар көп болған)
- Бет тану - қара нәсілді нашар танитды

**Шешім:** Теңдестірілген деректер.

### 25. ЖИ жүйесінде әділдікті қамтамасыз ету жолдарын сипаттаңыз
**Жауап:**
1. **Әртүрлі деректер** - барлық топтардан
2. **Аудит** - модельді тексеру
3. **Fairness метрикалары** - топтар бойынша дәлдікті салыстыру

**Мысал:** Несие моделі: ер адамдарға 90% дәлдік, әйелдерге 85% → Bias бар, түзету керек.

### 26. NLP (Natural Language Processing) мәнін және қолдану саласын түсіндіріңіз
**Жауап:** Табиғи тілді өңдеу - мәтінмен жұмыс.

**Қолдану:**
- Аударма (Google Translate)
- Чат-боттар
- Сезім талдау (пікірлер оң/теріс?)

**Мысал:** "Тамақ өте жақсы!" → Модель → Оң сезім (95%)

### 27. Тіл өңдеуде алдын ала өңдеу (Text Preprocessing) әдістерін атаңыз және сипаттаңыз
**Жауап:**
- **Tokenization** - сөздерге бөлу
- **Lowercasing** - кіші әріпке
- **Stopwords removal** - қажетсіз сөздер (және, немесе)
- **Stemming** - түбір сөз

**Мысал:** "Мен үйге барамын!" → ['мен', 'үй', 'бар']

### 28. Тар ЖИ мен жалпы ЖИ түрлерінің айырмашылықтарын түсіндіріңіз
**Жауап:**
- **Тар AI (Narrow AI)** - бір нәрсені жақсы істейді. Мысал: шахмат ойнайды.
- **Жалпы AI (General AI)** - адам сияқты әр түрлі нәрселерді істейді. Әлі жоқ.

**Мысал:** Siri - тар AI (тек команда орындайды), адам - жалпы интеллект.

### 29. Машиналық оқытуға негізделген жүйелердің ерекшеліктерін сипаттаңыз
**Жауап:**
- **Деректерден үйренеді** - бағдарлама жазбаймыз
- **Өздігінен жақсарады** - көбірек деректермен
- **Паттернді табады** - адам көрмейтін заңдылықтар

**Мысал:** Спам фильтр - 10000 email-ден үйреніп, жаңа спамды 98% дәлдікпен табады.

### 30. Машиналық оқытудың негізгі түрлері: Supervised, Unsupervised, Reinforcement Learning сипаттамасын жазу
**Жауап:**
- **Supervised** - белгіленген деректермен (кіріс+жауап). Мысал: сурет + "мысық"
- **Unsupervised** - белгісіз, өзі топтайды. Мысал: клиенттерді кластерлеу
- **Reinforcement** - сыйақы/жаза арқылы. Мысал: ойын ойнап үйренеді

## 2-блок сұрақтары

### 31. Векторизацияның мәтінді сандық түрде модельге беру үшін қажеттілігін түсіндіріңіз
**Жауап:** Компьютер тек сандарды түсінеді, сөздер жұмыс істемейді.

**Әдістер:**
- **Bag of Words** - сөз саны
- **TF-IDF** - маңызды сөздерді табу
- **Word2Vec** - сөздің векторы

**Мысал:** "Мысық жақсы" → [0.2, 0.8, 0.1, 0.3...] (128 сан)

### 32. MLP архитектурасындағы қабаттардың рөлін сипаттаңыз
**Жауап:**
- **Input layer** - деректерді қабылдайды
- **Hidden layers** - ерекшеліктерді үйренеді (әрқайсысы күрделене түседі)
- **Output layer** - соңғы шешім

**Мысал:** [784 кіріс] → [128 жасырын] → [64 жасырын] → [10 шығыс (0-9 цифрлары)]

### 33. Input, hidden және output қабаттарының байланысын баяндаңыз
**Жауап:** Әрбір қабат келесіге толық байланысқан (fully connected):
- Input → Hidden: W1 салмақтары
- Hidden → Output: W2 салмақтары

**Мысал:** 4 кіріс, 3 жасырын, 2 шығыс → 4×3=12 + 3×2=6 = 18 салмақ

### 34. TensorFlow моделінде compile параметрлерін талдаңыз
**Жауап:**
```python
model.compile(
    optimizer='adam',  # Салмақтарды қалай жаңарту (adam - ең жақсы)
    loss='categorical_crossentropy',  # Қателік функциясы
    metrics=['accuracy']  # Не өлшейміз
)
```

**Мысал:** Классификация → optimizer=adam, loss=crossentropy. Регрессия → loss=mse

### 35. Классификация және регрессия модельдерінің айырмашылығын түсіндіріңіз
**Жауап:**
- **Классификация** - категория болжау (мысық/ит)
  - Шығыс: Softmax, Loss: CrossEntropy
- **Регрессия** - сан болжау (баға, температура)
  - Шығыс: Linear, Loss: MSE

**Мысал:** Класс: Сурет → мысық (0) немесе ит (1). Регрессия: Үй → 200000$

### 36. Model.fit() функциясының жұмыс логикасын сипаттаңыз
**Жауап:**
```python
model.fit(X_train, y_train,  # Оқыту деректері
          epochs=10,             # Қанша рет қайталау
          batch_size=32,         # Бір рет қанша дерек
          validation_split=0.2)  # Тексеру үшін 20%
```

**Процесс:** Әр epoch-та барлық деректерді batch-қа бөліп оқытады, validation-да тексереді.

**Мысал:** 1000 дерек, batch=32 → 1000/32=31 қадам бір epoch-та

### 37. Batch Normalization принципін түсіндіріңіз
**Жауап:** Әрбір қабаттан кейін деректерді қалыпқа келтіреді (орташа=0, дисперсия=1).

**Артықшылығы:**
- Жылдам оқытады
- Тұрақтырақ

**Мысал:** Қабат шығысы [100, 200, 150] → BatchNorm → [-1, 1, 0]

### 38. Activation функциясын таңдаудың критерийлерін сипаттаңыз
**Жауап:**
- **ReLU** - жасырын қабаттарда (жылдам, default)
- **Sigmoid** - бинарлы классификация шығысында (0-1)
- **Softmax** - көп класс шығысында (ықтималдық)
- **Tanh** - RNN-де

**Мысал:** Жасырын: ReLU, Шығыс (10 класс): Softmax

### 39. TensorFlow модельдерін fine-tuning тәсілімен жақсартуды баяндаңыз
**Жауап:** Дайын моделді жаңа тапсырмаға бейімдеу:
1. Дайын модельді жүктеу (ImageNet-пен оқытылған)
2. Соңғы қабатты ауыстыру (өз класстарыңыз)
3. Алғашқы қабаттарды "freeze" ету
4. Тек соңғы қабаттарды оқыту

**Мысал:** ResNet50 (ImageNet) → өз иттерді классификациялау (10 түр)

### 40. Callbacks объектілерінің рөлін түсіндіріңіз
**Жауап:** Оқыту процесін бақылау және басқару:
- **EarlyStopping** - жақсармаса тоқтату
- **ModelCheckpoint** - ең жақсы модельді сақтау
- **ReduceLROnPlateau** - оқыту жылдамдығын төмендету

**Мысал:**
```python
callbacks = [EarlyStopping(patience=5, monitor='val_loss')]
model.fit(..., callbacks=callbacks)
```
Validation loss 5 epoch жақсармаса тоқтайды.

### 41. Learning rate параметрін баптау әдістерін сипаттаңыз
**Жауап:**
- **Тым үлкен** - конвергенция жоқ (секіреді)
- **Тым кіші** - өте баяу
- **Оңтайлы** - 0.001 (default), 0.01 немесе 0.0001

**Стратегия:** Үлкеннен бастап азайту (lr=0.01 → 0.001)

**Мысал:** lr=0.1 → loss секіреді. lr=0.001 → біртіндеп төмендейді.

### 42. TensorFlow-де custom loss функциясын құру жолдарын түсіндіріңіз
**Жауап:**
```python
def custom_loss(y_true, y_pred):
    # Өз логикаңыз
    return tf.reduce_mean(tf.square(y_true - y_pred))

model.compile(loss=custom_loss)
```

**Мысал:** Үлкен қателерді көбірек жазалау үшін.

### 43. MLP желісін нақты деректермен үйрету мысалын баяндаңыз
**Жауап:**
```python
# MNIST цифрларын тану
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.fit(X_train, y_train, epochs=10, batch_size=32)
```

**Нәтиже:** 60000 сурет × 10 epoch → 98% дәлдік

### 44. Нейрондық салмақтарды (weights) инициализациялау тәсілдерін сипаттаңыз
**Жауап:**
- **Zero** - барлығы 0 (нашар, үйренбейді)
- **Random** - кездейсоқ кіші сандар
- **Xavier** - ReLU үшін
- **He** - ReLU үшін жақсырақ

**Формула:** He: w ~ N(0, 2/n) где n - кіріс саны

**Мысал:** 100 кіріс → σ = sqrt(2/100) = 0.14

### 45. Gradient clipping ұғымын түсіндіріңіз
**Жауап:** Градиент тым үлкен болса, шектеу қою (exploding gradient проблемасын шешу).

**Әдіс:** Градиент > threshold болса, кішірейту.

**Мысал:** Threshold=1.0, Градиент=5 → 5/5 × 1.0 = 1.0

### 46. Нейрондық желілерде regularization түрлерін баяндаңыз
**Жауап:**
- **L1** - кейбір салмақтарды 0-ге айналдырады
- **L2 (Ridge)** - салмақтарды кішірейтеді
- **Dropout** - кездейсоқ нейрондарды өшіреді

**Мақсат:** Overfitting-ті болдырмау.

**Мысал:** Dropout(0.5) → 50% нейрондар әр батчта өшеді.

### 47. TensorFlow-де модель құрылымын визуалдау тәсілдерін сипаттаңыз
**Жауап:**
```python
model.summary()  # Мәтінмен
tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)
```

**Шығыс:**
```
Layer (type)      Output Shape    Param #
Dense (Dense)     (None, 128)     100480
Dense (Dense)     (None, 10)      1290
```

### 48. Model.evaluate() функциясын қолдану ерекшеліктерін түсіндіріңіз
**Жауап:** Тест деректеріндегі модель сапасын өлшейді.

```python
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Дәлдік: {accuracy*100:.2f}%')
```

**Мысал:** 10000 тест дерек → loss=0.15, accuracy=0.97 (97%)

### 49. Модельдің тиімділігін арттыру стратегияларын баяндаңыз
**Жауап:**
1. **Көбірек деректер** - data augmentation
2. **Жақсы архитектура** - қабаттар саны, нейрондар
3. **Regularization** - dropout, L2
4. **Hyperparameter tuning** - lr, batch_size
5. **Transfer learning** - дайын модельдер

**Мысал:** 90% → Data augmentation → 93% → Dropout → 95%

### 50. Dataset-ті shuffle және batch әдістерімен өңдеуді сипаттаңыз
**Жауап:**
- **Shuffle** - деректерді араластыру (bias жоқ)
- **Batch** - кіші топтарға бөлу

```python
dataset = dataset.shuffle(buffer_size=1000).batch(32)
```

**Мысал:** 1000 дерек → Shuffle → [32, 32, 32...] batch-тар

### 51. Custom layer құру процесін түсіндіріңіз
**Жауап:**
```python
class MyLayer(tf.keras.layers.Layer):
    def build(self, input_shape):
        self.w = self.add_weight(shape=(input_shape[-1], 64))
    
    def call(self, inputs):
        return tf.matmul(inputs, self.w)
```

**Қолдану:** Стандарт емес операциялар үшін.

### 52. Sequential және Functional API айырмашылығын баяндаңыз
**Жауап:**
- **Sequential** - қарапайым, бір жол
  ```python
  model = Sequential([Dense(128), Dense(10)])
  ```
- **Functional** - күрделі (бірнеше кіріс/шығыс, skip connections)
  ```python
  inputs = Input(shape=(784,))
  x = Dense(128)(inputs)
  outputs = Dense(10)(x)
  model = Model(inputs, outputs)
  ```

**Мысал:** Multi-input model → Functional API қолданамыз

### 53. Keras.layers қолданудың негізгі принциптерін сипаттаңыз
**Жауап:** Әрбір layer - деректерді түрлендіреді:
- **Dense** - толық байланысқан
- **Conv2D** - сурет ерекшелігі
- **LSTM** - тізбек деректері
- **Dropout** - regularization

**Синтаксис:** `layer_name(parameters)(input)`

**Мысал:** `Dense(64, activation='relu')(x)`

### 54. TensorFlow Serving құралын түсіндіріңіз
**Жауап:** Модельді production-да қолдану үшін сервер.

**Процесс:**
1. Модельді сақтау: `model.save('my_model')`
2. TF Serving іске қосу
3. REST/gRPC арқылы болжам жасау

**Мысал:** Веб-сайт → REST запрос → TF Serving → болжам

### 55. Модельді веб-қосымшада пайдалану тәсілдерін баяндаңыз
**Жауап:**
1. **Backend API** - Flask/FastAPI + модель
2. **TensorFlow.js** - браузерде тікелей
3. **Cloud** - AWS/Google Cloud ML

**Мысал Flask:**
```python
@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    prediction = model.predict(data)
    return jsonify({'result': prediction})
```

### 56. TensorFlow Lite көмегімен мобильді құрылғыда модельді қолдануды түсіндіріңіз
**Жауап:** Модельді кішірейтіп, телефонға салу:
1. Конверт: `converter = tf.lite.TFLiteConverter.from_saved_model('model')`
2. Оптимизация: quantization
3. .tflite файл → Android/iOS

**Артықшылық:** Жылдам, интернетсіз жұмыс істейді.

**Мысал:** Камера → тікелей объект тану (телефонда)

### 57. TensorFlow.js көмегімен браузерде модельді іске асыру жолдарын сипаттаңыз
**Жауап:**
```javascript
// Модельді жүктеу
const model = await tf.loadLayersModel('model.json');

// Болжам
const input = tf.tensor2d([[1, 2, 3, 4]]);
const output = model.predict(input);
```

**Қолдану:** Веб-сайтта тікелей ML (сурет тану, бет тану).

### 58. Веб-қосымшада MLP моделін API арқылы біріктіру тәсілін баяндаңыз
**Жауап:**
**Backend (Python):**
```python
from flask import Flask, request, jsonify
app = Flask(__name__)
model = load_model('mlp_model.h5')

@app.route('/predict', methods=['POST'])
def predict():
    data = np.array(request.json['input'])
    pred = model.predict(data)
    return jsonify({'prediction': pred.tolist()})
```

**Frontend (JavaScript):**
```javascript
fetch('/predict', {
    method: 'POST',
    body: JSON.stringify({input: [1,2,3,4]})
}).then(r => r.json())
```

### 59. Шешім ағашы (Decision Tree) алгоритмінің негізгі идеясы мен жұмыс принципін түсіндіріңіз
**Жауап:** Сұрақтар арқылы шешім:
1. Ең маңызды ерекшелікті таңдау (Information Gain)
2. Деректерді бөлу
3. Әр бұтақты қайталау
4. Таза класс болғанша

**Мысал:** Несие беру ма? → Табыс>5000? → ИӘ → Жас>25? → ИӘ → НЕСИЕ БЕР

### 60. Confusion Matrix құрылымын (TP, TN, FP, FN) және олардың мағынасын түсіндіріңіз
**Жауап:**
|               | Болжам: ИӘ | Болжам: ЖОҚ |
|---------------|-----------|-------------|
| Нақты: ИӘ     | TP        | FN          |
| Нақты: ЖОҚ    | FP        | TN          |

- **TP (True Positive)** - дұрыс табылды (Спам → Спам деп айтты)
- **TN (True Negative)** - дұрыс жоққа шығарды (Спам емес → Спам емес)
- **FP (False Positive)** - жалған тривога (Спам емес → Спам деді) 
- **FN (False Negative)** - жіберіп алды (Спам → Спам емес деді)

**Метрикалар:**
- Accuracy = (TP+TN)/(TP+TN+FP+FN)
- Precision = TP/(TP+FP)
- Recall = TP/(TP+FN)

## 3-блок сұрақтары (Практикалық есептеулер)

### 61. Жоғалған мәндерді анықтап, оларды өңдеу әдістерін түсіндіріңіз
**Жауап:** Жоғалған мәндерді (NaN, None) табу:
```python
df.isnull().sum()  # Әр бағанда қанша NaN
```

**Өңдеу әдістері:**
1. **Жою** - `df.dropna()` (аз болса)
2. **Толтыру** - `df.fillna(орташа)` (көп болса)
3. **Болжау** - ML арқылы

**Мысал:**
```
Age: [25, NaN, 30] → Орташа = (25+30)/2 = 27.5 → [25, 27.5, 30]
```

### 62. Деректердің Min-Max нормализациясы қалай орындалатынын формуламен және шартты мысалмен жазыңыз
**Жауап:** Формула: **X_norm = (X - X_min) / (X_max - X_min)**

Мәндерді [0, 1] аралығына келтіреді.

**Мысал:**
```
Табыс: [50000, 100000, 150000]
X_min = 50000, X_max = 150000

50000: (50000-50000)/(150000-50000) = 0/100000 = 0
100000: (100000-50000)/(150000-50000) = 50000/100000 = 0.5
150000: (150000-50000)/(150000-50000) = 100000/100000 = 1

Нәтиже: [0, 0.5, 1]
```

### 63. Z-score стандарттау әдісін қолдану қадамдарын берілген мини-кесте мысалында қолмен есептеп көрсетіңіз
**Жауап:** Формула: **Z = (X - μ) / σ**
- μ = орташа мән
- σ = стандартты ауытқу

**Мысал:**
```
Ұпайлар: [10, 20, 30]

1. Орташа: μ = (10+20+30)/3 = 20
2. Дисперсия: σ² = [(10-20)² + (20-20)² + (30-20)²]/3 = [100+0+100]/3 = 66.67
3. Стандартты ауытқу: σ = √66.67 = 8.16

Z-score:
10: (10-20)/8.16 = -1.22
20: (20-20)/8.16 = 0
30: (30-20)/8.16 = 1.22

Нәтиже: [-1.22, 0, 1.22]
```

### 64. Категориялық «City» айнымалысын One-Hot Encoding әдісі арқылы кодтаудың нәтижесін кесте түрінде көрсетіңіз
**Жауап:** Әрбір категорияға жеке баған.

**Бастапқы:**
| ID | City    |
|----|---------|
| 1  | Алматы  |
| 2  | Астана  |
| 3  | Алматы  |

**One-Hot Encoding:**
| ID | City_Алматы | City_Астана |
|----|-------------|-------------|
| 1  | 1           | 0           |
| 2  | 0           | 1           |
| 3  | 1           | 0           |

### 65. Outlier деректерді анықтаудың IQR әдісін толық қадамдарын көрсетіп талдаңыз
**Жауап:** **IQR (Interquartile Range) = Q3 - Q1**

**Қадамдар:**
1. Деректерді сұрыптау
2. Q1 (25%) және Q3 (75%) табу
3. IQR = Q3 - Q1
4. Төменгі шекара = Q1 - 1.5×IQR
5. Жоғарғы шекара = Q3 + 1.5×IQR
6. Шекарадан тыс = Outlier

**Мысал:**
```
Деректер: [10, 12, 15, 18, 20, 25, 100]

Q1 = 12, Q3 = 25
IQR = 25 - 12 = 13
Төменгі = 12 - 1.5×13 = -7.5
Жоғарғы = 25 + 1.5×13 = 44.5

100 > 44.5 → Outlier!
```

### 66. Берілген деректер жиынындағы корреляцияны есептеу әдісін жазу және кесте бойынша нәтижені түсіндіріңіз
**Жауап:** Формула: **r = Cov(X,Y) / (σ_X × σ_Y)**

Мәні: [-1, 1]
- r = 1: оң корреляция (бірге өседі)
- r = -1: теріс корреляция (бірі өссе, екінші кемиді)
- r = 0: корреляция жоқ

**Мысал:**
```
Оқу сағаты (X): [1, 2, 3, 4, 5]
Ұпай (Y): [50, 55, 65, 70, 80]

Корреляция ≈ 0.98 → Күшті оң байланыс (көп оқыса, ұпай жоғары)
```

### 67. Фичерлерді шкалалау не үшін қажет екенін, робототехника немесе инженерлік есеп мысалымен түсіндіріңіз
**Жауап:** **Мақсат:** Барлық ерекшеліктерді бір масштабқа келтіру.

**Себебі:** Үлкен мәндер кіші мәндерді "басып кетеді".

**Робототехника мысалы:**
```
Қашықтық (см): [5, 10, 15] - кіші сандар
Бұрыш (градус): [90, 180, 270] - үлкен сандар

Scaling жоқ: ML модель қашықтықты елемейді (тым кіші)
Scaling бар: [0.33, 0.67, 1.0] және [0.33, 0.67, 1.0] - әділ
```

### 68. Берілген 5 жолдан тұратын кесте үшін Label Encoding нәтижесін қолмен жазып шығыңыз
**Жауап:** Әрбір категорияға сан береді.

**Бастапқы:**
| ID | Деңгей   |
|----|----------|
| 1  | Төмен    |
| 2  | Орташа   |
| 3  | Жоғары   |
| 4  | Орташа   |
| 5  | Төмен    |

**Label Encoding:**
| ID | Деңгей   | Код |
|----|----------|-----|
| 1  | Төмен    | 0   |
| 2  | Орташа   | 1   |
| 3  | Жоғары   | 2   |
| 4  | Орташа   | 1   |
| 5  | Төмен    | 0   |

### 69. Дисбалансты деректер мәселесін анықтап, оны түзетудің екі жолын түсіндіріңіз (oversampling/undersampling)
**Жауап:** **Мәселе:** Бір класс көп, екіншісі аз.

**Мысал:** Спам тану - 95% спам емес, 5% спам → модель "спам емес" деп жаттап алады.

**Шешімдері:**
1. **Oversampling** - аз классты көбейту (SMOTE)
   - 5 спам → 100 спам (синтетикалық)
2. **Undersampling** - көп классты азайту
   - 95 спам емес → 5 спам емес

**Нәтиже:** 50% спам, 50% спам емес - теңдестірілді

### 70. Train-test split операциясының мақсатын және модель сапасына әсерін нақты мысалмен баяндаңыз
**Жауап:** **Мақсат:** Модельдің жалпылау қабілетін тексеру.

**Әдіс:**
- 80% оқыту (train) - модельді үйрету
- 20% тест (test) - модельді тексеру

**Мысал:**
```python
train_test_split(X, y, test_size=0.2, random_state=42)
```

**Нәтиже:**
- Train: 98% дәлдік, Test: 96% → ЖАҚСЫ (generalize)
- Train: 98%, Test: 60% → НАШАР (overfitting)

### 71. KNN алгоритмінде берілген нүктеге ең жақын 3 көршіні Евклид қашықтығы әдісімен қолмен есептеңіз
**Жауап:** Формула: **d = √[(x₂-x₁)² + (y₂-y₁)²]**

**Мысал:**
```
Жаңа нүкте: (5, 5)
Деректер:
A(2, 3) - қызыл
B(6, 7) - қызыл
C(1, 8) - көк
D(7, 4) - қызыл

Қашықтық есептеу:
A: √[(5-2)² + (5-3)²] = √[9+4] = √13 = 3.61
B: √[(5-6)² + (5-7)²] = √[1+4] = √5 = 2.24
C: √[(5-1)² + (5-8)²] = √[16+9] = √25 = 5.0
D: √[(5-7)² + (5-4)²] = √[4+1] = √5 = 2.24

K=3 ең жақын: B(2.24), D(2.24), A(3.61)
Класс: 3 қызыл, 0 көк → Жауап: ҚЫЗЫЛ
```

### 72. KNN классификациясында k параметрі өскендегі шешім шекараларының өзгерісін мысалмен түсіндіріңіз
**Жауап:**
- **k кіші (k=1)** - шекара өте күрделі, overfitting
- **k үлкен (k=100)** - шекара тегіс, underfitting

**Мысал:**
```
k=1: Тек 1 ең жақын → шулы деректерге сезімтал
k=3: 3 ең жақын → жақсы баланс
k=50: 50 көршіні қарайды → тым жалпыланған, дәлдік төмен
```

**Қорытынды:** k=√n (n - деректер саны) - жақсы бастапқы мән.

### 73. Decision Tree алгоритміндегі энтропияны және Information Gain-ді берілген шағын кесте мысалы арқылы қолмен есептеңіз
**Жауап:** **Энтропия:** H = -Σ p_i × log₂(p_i)

**Мысал:**
```
Dataset: 6 ИӘ, 4 ЖОҚ (Несие беру)

Энтропия = -[6/10 × log₂(6/10) + 4/10 × log₂(4/10)]
         = -[0.6 × (-0.74) + 0.4 × (-1.32)]
         = -[-0.44 - 0.53]
         = 0.97

Табыс > 5000 бойынша бөлу:
Иә (Табыс>5000): 5 ИӘ, 1 ЖОҚ → H = 0.65
Жоқ: 1 ИӘ, 3 ЖОҚ → H = 0.81

Information Gain = 0.97 - [6/10×0.65 + 4/10×0.81] = 0.97 - 0.71 = 0.26
```
Үлкен Gain = жақсы бөліну!

### 74. Decision Tree-де overfitting пен pruning арасындағы байланысты мысалмен көрсетіңіз
**Жауап:**
- **Overfitting** - ағаш тым терең, әрбір жапырақта 1 дерек
- **Pruning** - ағашты қысқарту

**Pre-pruning:** max_depth = 5 қою
**Post-pruning:** толық өсіргеннен кейін маңызсыз бұтақтарды кесу

**Мысал:**
```
Overfitting: 20 деңгей тереңдік, Train=100%, Test=70%
Pruning: 5 деңгей, Train=95%, Test=90% - ЖАҚСЫ!
```

### 75. Логистикалық регрессиядағы sigmoid функциясының мәнін берілген x үшін қолмен есептеңіз
**Жауап:** Формула: **σ(x) = 1 / (1 + e^(-x))**

**Мысал 1:** x = 0
```
σ(0) = 1 / (1 + e^0) = 1 / (1 + 1) = 1/2 = 0.5
```

**Мысал 2:** x = 2
```
σ(2) = 1 / (1 + e^(-2)) = 1 / (1 + 0.135) = 1 / 1.135 = 0.88
```

**Мысал 3:** x = -2
```
σ(-2) = 1 / (1 + e^2) = 1 / (1 + 7.39) = 1 / 8.39 = 0.12
```

**Интерпретация:** x өссе, σ(x) → 1 (ИӘ), x кемісе → 0 (ЖОҚ)

### 76. Logistic Regression шешім шекарасының мағынасын екі класқа арналған мысал арқылы түсіндіріңіз
**Жауап:** **Шекара:** P = 0.5 (ықтималдық)

**Мысал:** Email спам ма?
```
Логистикалық функция: z = w₁×сөз_саны + w₂×сілтеме_саны

Email 1: z = 3 → σ(3) = 0.95 > 0.5 → СПАМ
Email 2: z = -2 → σ(-2) = 0.12 < 0.5 → СПАМ ЕМЕС
Email 3: z = 0 → σ(0) = 0.5 (ШЕКАРАДА)
```

**Шекара:** Әдетте P=0.5, бірақ өзгертуге болады (P=0.7 - консервативті).

### 77. Confusion Matrix кестесін беріп, Accuracy, Precision, Recall көрсеткіштерін қолмен есептеңіз
**Жауап:**
**Берілген:**
|               | Болжам: Оң | Болжам: Теріс |
|---------------|-----------|---------------|
| Нақты: Оң     | 80 (TP)   | 20 (FN)       |
| Нақты: Теріс  | 10 (FP)   | 90 (TN)       |

**Есептеу:**
1. **Accuracy** = (TP + TN) / Барлығы = (80 + 90) / 200 = 170/200 = **0.85 (85%)**

2. **Precision** = TP / (TP + FP) = 80 / (80 + 10) = 80/90 = **0.89 (89%)**
   - "Оң дегеніміздің қаншасы шын оң?"

3. **Recall** = TP / (TP + FN) = 80 / (80 + 20) = 80/100 = **0.80 (80%)**
   - "Барлық оң мәндердің қаншасын таптық?"

### 78. ROC curve не көрсететінін және AUC интерпретациясын нақты инженерлік мысалмен жазыңыз
**Жауап:** **ROC (Receiver Operating Characteristic)**
- X осі: False Positive Rate
- Y осі: True Positive Rate

**AUC (Area Under Curve):**
- AUC = 1.0 - керемет модель
- AUC = 0.5 - кездейсоқ болжам

**Инженерлік мысал:** Ақаулы бөлшектерді анықтау
```
AUC = 0.95 → Модель 95% ақауларды дұрыс табады, 5% қате
AUC = 0.6 → Модель нашар, қолмен тексеру жақсырақ

Практикада: AUC > 0.8 - қолдануға жарайды
```

### 79. Көп классты классификацияда one-vs-rest стратегиясының жұмысын схемамен суреттеп баяндаңыз
**Жауап:** **Идея:** Әрбір класс үшін жеке бинарлы классификатор.

**Мысал:** 3 класс (Мысық, Ит, Құс)
```
Классификатор 1: Мысық vs (Ит + Құс)
Классификатор 2: Ит vs (Мысық + Құс)
Классификатор 3: Құс vs (Мысық + Ит)

Жаңа сурет:
- Класс 1: P(Мысық) = 0.8
- Класс 2: P(Ит) = 0.3
- Класс 3: P(Құс) = 0.1

Нәтиже: МЫСЫҚ (ең жоғары ықтималдық)
```

### 80. Модельдің generalization қабілетіне әсер ететін үш факторды түсіндіріп жазыңыз
**Жауап:**
1. **Деректер саны** - көп = жақсырақ generalize
   - 100 дерек → overfitting, 10000 → жақсы

2. **Модель күрделілігі** - тым күрделі = overfitting
   - 100 параметр (10 деректен) = нашар, 10 параметр = жақсы

3. **Regularization** - салмақтарды шектеу
   - L2, Dropout → жақсарады

**Мысал:** 1000 дерек, қарапайым модель, Dropout=0.3 → Test дәлдік жоғары

### 81. K-means алгоритмінде екі кластер үшін алғашқы центрлер мен үш нүктенің қашықтығын есептеп, кластерге тағайындаңыз
**Жауап:**
**Берілген:**
- Центр 1: C1(2, 2)
- Центр 2: C2(8, 8)
- Нүктелер: A(3, 4), B(7, 9), C(1, 2)

**Қашықтық есептеу:**
```
A(3,4) дейін:
- C1: √[(3-2)² + (4-2)²] = √[1+4] = 2.24
- C2: √[(3-8)² + (4-8)²] = √[25+16] = 6.40
→ A ∈ Кластер 1

B(7,9) дейін:
- C1: √[(7-2)² + (9-2)²] = √[25+49] = 8.60
- C2: √[(7-8)² + (9-8)²] = √[1+1] = 1.41
→ B ∈ Кластер 2

C(1,2) дейін:
- C1: √[(1-2)² + (2-2)²] = √[1+0] = 1.0
- C2: √[(1-8)² + (2-8)²] = √[49+36] = 9.22
→ C ∈ Кластер 1
```

**Нәтиже:** Кластер 1: {A, C}, Кластер 2: {B}

### 82. K-means алгоритмінің 1 итерациясын толық жазу: жаңа центрлерді қайта есептеңіз
**Жауап:** Жоғарыдағы мысалдың жалғасы.

**Ескі кластерлер:**
- Кластер 1: A(3,4), C(1,2)
- Кластер 2: B(7,9)

**Жаңа центрлерді есептеу:**
```
C1_жаңа = [(3+1)/2, (4+2)/2] = [2, 3]
C2_жаңа = [7, 9] (бір нүкте)

Нәтиже:
C1: (2, 2) → (2, 3)
C2: (8, 8) → (7, 9)
```

Содан кейін 2-итерация: қайта тағайындау...

### 83. K-параметрін таңдауда elbow әдісінің логикасын түсіндіріп, графикалық мысалды сөзбен сипаттаңыз
**Жауап:** **Elbow (Шынтақ) әдісі:** WCSS (Within-Cluster Sum of Squares) графигінен оңтайлы K табу.

**Процесс:**
1. K=1,2,3...10 үшін K-means іске қосу
2. Әр K үшін WCSS есептеу
3. График салу

**График:**
```
WCSS
  ↑
  |     *
  |      *
  |        *
  |           *___*___*___* ("шынтақ")
  +-----------------------> K
     1  2  3  4  5  6  7
```

**Түсіндірме:** K=3-те график тік төмендеуден көлденең өтеді - бұл оңтайлы нүкте. K>3 болса, WCSS азырақ кемиді (артыққа соғады).

### 84. PCA әдісіндегі дисперсияны есептеу және ковариация матрицасының рөлін түсіндіріңіз
**Жауап:** **PCA** - өлшемді азайту (мысалы 10 → 2).

**Қадамдар:**
1. **Стандарттау** - деректерді қалыпқа келтіру
2. **Ковариация матрицасы** - ерекшеліктер арасындағы байланыс
   ```
   Cov(X,Y) = Σ(X-μₓ)(Y-μᵧ) / (n-1)
   ```
3. **Eigenvalues/Eigenvectors** - басты компоненттер
4. **Дисперсия** - әр компонент қанша ақпарат сақтайды

**Мысал:**
```
PC1: 80% дисперсия (ең маңызды)
PC2: 15% дисперсия
PC3: 5%

PC1+PC2 = 95% → Осы 2 компонент жеткілікті!
```

### 85. Өлшемді қысқартудың (PCA) артықшылықтарын инженерлік жүйедегі мысалмен сипаттаңыз
**Жауап:** **Артықшылықтар:**
1. **Визуализация** - 100D → 2D график
2. **Жылдамдық** - аз параметр = жылдам оқыту
3. **Overfitting азайту** - қарапайым модель

**Инженерлік мысал:** Датчиктерден деректер
```
100 датчик (температура, қысым, дірілдеу...) → PCA → 5 басты компонент
- Деректер саны: 100 → 5 (20 есе аз)
- Модель жылдамдығы: 10 сек → 0.5 сек
- Дәлдік: 95% (5% ақпарат жоғалтты, бірақ қолайлы)
```

### 86. "Банк" сөзі екі сөйлемде: "Банкте ақша алдым" және "Өзен банкінде отырдым". BERT пен One-Hot Encoding қалай өңдейтінін көрсетіңіз
**Жауап:**

**One-Hot Encoding:**
```
"Банк" → [0,0,1,0,0...] (бірдей вектор екі жағдайда да)
- Контекстті түсінбейді
- "Банк"=финанс және "Банк"=жер - бірдей!
```

**BERT:**
```
"Банкте ақша алдым" → "Банк" = [0.2, 0.9, 0.1...] (финанс векторы)
"Өзен банкінде отырдым" → "Банк" = [0.7, 0.1, 0.8...] (жер векторы)

- Контекстті түсінеді
- Әр "Банк" әр түрлі векторға айналады
```

**Қорытынды:** BERT күшті, сөздің мағынасын контексттен білді!

### 87. A(1,2) және B(4,6) нүктелері арасындағы Евклид қашықтығын есептеп көрсетіңіз
**Жауап:** Формула: **d = √[(x₂-x₁)² + (y₂-y₁)²]**

**Есептеу:**
```
A(1, 2), B(4, 6)

d = √[(4-1)² + (6-2)²]
  = √[3² + 4²]
  = √[9 + 16]
  = √25
  = 5
```

**Жауап: 5 бірлік**

### 88. PCA өлшемділікті азайтқанда деректердің қандай бөлігі жоғалуы мүмкін екенін мысалмен түсіндіріңіз
**Жауап:** **Жоғалатын:** Аз маңызды ақпарат (дисперсия төмен компоненттер).

**Мысал:**
```
100 ерекшелік:
- PC1-PC10: 90% дисперсия (БАСТЫ ақпарат)
- PC11-PC100: 10% дисперсия (шу, маңызсыз)

10 компонент қалдырсақ → 10% ақпарат жоғалады
```

**Практика:**
```
Бастапқы: 1000 өлшем → Модель дәлдігі 95%
PCA (50 компонент): 50 өлшем → Дәлдік 94%

Жоғалту: 1% дәлдік, алу: 20 есе жылдам оқыту!
```

### 89. Персептронның AND логикалық функциясын қалай үйренетінін қадаммен түсіндіріңіз
**Жауап:** **AND таблицасы:**
| X1 | X2 | Y |
|----|----|---|
| 0  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 0  | 0 |
| 1  | 1  | 1 |

**Оқыту:**
```
Бастапқы: w1=0.5, w2=0.5, bias=-0.7, lr=0.1

Кіріс (0,0):
Шығыс = 0*0.5 + 0*0.5 - 0.7 = -0.7 → 0 (дұрыс ✓)

Кіріс (1,1):
Шығыс = 1*0.5 + 1*0.5 - 0.7 = 0.3 → 1 (step function)
Қателік = 1 - 1 = 0 (дұрыс ✓)

Кіріс (1,0):
Шығыс = 1*0.5 + 0*0.5 - 0.7 = -0.2 → 0 (дұрыс ✓)
```

**Соңғы:** w1≈1, w2≈1, bias≈-1.5 → AND функциясы үйренілді!

### 90. ReLU мен Sigmoid функцияларының айырмасын және қай жағдайда қайсысы тиімді екенін мысалмен көрсетіңіз
**Жауап:**

**Формулалар:**
- **ReLU:** f(x) = max(0, x) = {x егер x>0, 0 егер x≤0}
- **Sigmoid:** σ(x) = 1/(1+e^(-x))

**Айырмашылықтар:**
| Параметр | ReLU | Sigmoid |
|----------|------|---------|
| Диапазон | [0, ∞) | [0, 1] |
| Жылдамдық | Жылдам | Баяу |
| Градиент жоғалу | Жоқ | Бар |

**Қолдану:**
```
ReLU - Жасырын қабаттарда:
model = Sequential([
    Dense(128, activation='relu'),  # Жасырын
    Dense(1, activation='sigmoid')  # Шығыс (бинарлы)
])

Sigmoid - Шығыс қабатта (ықтималдық 0-1):
- Спам фильтр: P(спам) = 0.85
- Аурудың ықтималдығы: P(дерт) = 0.12
```

**Мысал:**
```
x = -2: ReLU(-2) = 0, Sigmoid(-2) = 0.12
x = 0: ReLU(0) = 0, Sigmoid(0) = 0.5
x = 3: ReLU(3) = 3, Sigmoid(3) = 0.95
```

---

## Қорытынды

✅ Барлығы **90 сұрақ** жауаппен дайындалды  
✅ Қазақ тілінде қарапайым түсіндірмелер  
✅ Әрбір сұраққа нақты мысалдар  
✅ Практикалық есептеулер қолмен көрсетілді  

**Сәттілік сессияда! 🎓**

